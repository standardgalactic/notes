Cross-document CR refers to the sub-task where the 
mentions refer to the same entity or event might be across 
multiple documents. CDML [158] proposes a cross docu- 
ment language modeling method which pre-trains a Long- 
former [218] encoder on concatenated related documents 
and employs an MLP for binary classif i cation to determine 
whether a pair of mentions is coreferent or not. CrossCR 
[159] utilizes an end-to-end model for cross-document coref- 
erence resolution which pre-trained the mention scorer on 
gold mention spans and uses a pairwise scorer to compare 
mentions with all spans across all documents. CR-RL [160] 
proposes an actor-critic deep reinforcement learning-based 
coreference resolver for cross-document CR. 
5.3.3Relation Extraction (RE) 
Relation extraction involves identifying semantic relation- 
ships between entities mentioned in natural language text. 
There are two types of relation extraction methods, i.e. 
sentence-level RE and document-level RE, according to the 
scope of the text analyzed. 
Sentence-level RE focuses on identifying relations be- 
tween entities within a single sentence. Peng et al. [161] and 
TRE [219] introduce LLM to improve the performance of 
relation extraction models. BERT-MTB [220] learns relation 
representations based on BERT by performing the matching- 
the-blanks task and incorporating designed objectives for 
relation extraction. Curriculum-RE [162] utilizes curriculum 
learning to improve relation extraction models by gradu- 
ally increasing the diff i culty of the data during training. 
RECENT [221] introduces SpanBERT and exploits entity 
type restriction to reduce the noisy candidate relation types. 
Jiewen [222] extends RECENT by combining both the entity 
information and the label information into sentence-level 
embeddings, which enables the embedding to be entity- 
label aware. 
Document-level RE (DocRE) aims to extract relations 
between entities across multiple sentences within a docu- 
ment. Hong et al. [223] propose a strong baseline for DocRE 
by replacing the BiLSTM backbone with LLMs. HIN [224] 
use LLM to encode and aggregate entity representation at 
different levels, including entity, sentence, and document 
levels. GLRE [225] is a global-to-local network, which uses 
LLM to encode the document information in terms of entity 
global and local representations as well as context relation 
representations. SIRE [226] uses two LLM-based encoders to 
extract intra-sentence and inter-sentence relations. LSR [227] 
and GAIN [228] propose graph-based approaches which 
induce graph structures on top of LLM to better extract 
relations. DocuNet [229] formulates DocRE as a semantic 
segmentation task and introduces a U-Net [230] on the LLM 
encoder to capture local and global dependencies between 
entities. ATLOP [231] focuses on the multi-label problems 
in DocRE, which could be handled with two techniques, 
i.e., adaptive thresholding for classif i er and localized con- 
text pooling for LLM. DREEAM [163] further extends and 
improves ATLOP by incorporating evidence information. 
5.3.4End-to-End KG Construction 
Currently, researchers are exploring the use of LLMs for 
end-to-end KG construction. Kumar et al. [97] propose a 
Brarck Obama 
PoliticianOf USA 
Honolulu 
BornIn LocatedIn 
CapitalOf 
MarriedTo 
Michelle 
Obama 
LiveIn 
Construct KGs 
LLMs 
Obama born in [MASK] 
Honolulu is located in [MASK] 
USA's capital is [MASK] 
Cloze Question (Obama, BornIn, Honolulu) 
(Honolulu, LocatedIn, USA) 
(Washingto D.C., CapitalOf, USA) Washingto 
D.C. 
Distilled Triples 
Fig. 22. The general framework of distilling KGs from LLMs. 
unif i ed approach to build KGs from raw text, which con- 
tains two LLMs powered components. They i rst i netune a 
LLM on named entity recognition tasks to make it capable 
of recognizing entities in raw text. Then, they propose 
another “2-model BERT” for solving the relation extraction 
task, which contains two BERT-based classif i ers. The i rst 
classif i er learns the relation class whereas the second binary 
classif i er learns the direction of the relations between the 
two entities. The predicted triples and relations are then 
used to construct the KG. Guo et al. [164] propose an end- 
to-end knowledge extraction model based on BERT, which 
can be applied to construct KGs from Classical Chinese text. 
Grapher [41] presents a novel end-to-end multi-stage sys- 
tem. It i rst utilizes LLMs to generate KG entities, followed 
by a simple relation construction head, enabling eff i cient KG 
construction from the textual description. PiVE [165] pro- 
poses a prompting with an iterative verif i cation framework 
that utilizes a smaller LLM like T5 to correct the errors in 
KGs generated by a larger LLM (e.g., ChatGPT). To further 
explore advanced LLMs, AutoKG design several prompts 
for different KG construction tasks (e.g., entity typing, entity 
linking, and relation extraction). Then, it adopts the prompt 
to perform KG construction using ChatGPT and GPT-4. 
5.3.5Distilling Knowledge Graphs from LLMs 
LLMs have been shown to implicitly encode massive knowl- 
edge [14]. As shown in Fig. 22, some research aims to distill 
knowledge from LLMs to construct KGs. COMET [166] 
proposes a commonsense transformer model that constructs 
commonsense KGs by using existing tuples as a seed set of 
knowledge on which to train. Using this seed set, a LLM 
learns to adapt its learned representations to knowledge 
generation, and produces novel tuples that are high quality. 
Experimental results reveal that implicit knowledge from 
LLMs is transferred to generate explicit knowledge in com- 
monsense KGs. BertNet [167] proposes a novel framework 
for automatic KG construction empowered by LLMs. It re- 
quires only the minimal def i nition of relations as inputs and 
automatically generates diverse prompts, and performs an 
eff i cient knowledge search within a given LLM for consis- 
tent outputs. The constructed KGs show competitive quality, 
diversity, and novelty with a richer set of new and complex 
relations, which cannot be extracted by previous methods. 
West et al. [168] propose a symbolic knowledge distillation 
framework that distills symbolic knowledge from LLMs. 
They i rst i netune a small student LLM by distilling com- 
monsense facts from a large LLM like GPT-3. Then, the 
student LLM is utilized to generate commonsense KGs.