hello everyone um let's get things started [Applause] welcome to I think Kenneth is the fifth in the series yes the fifth seminar in the Berkeley AI series um thank you Ken for hosting the the whole series and setting this up um so honor to date have with us here John Schulman John is actually a Berkeley graduate graduate from Berkeley's PhD in 2016. is that right um from there co-founded open AI and most people say rest is history but not only that he also is the Chief Architect of chat GPT he is um the inventor of the modern uh deep learning based policy Grant algorithms including translation policy optimization which he did at Berkeley together with Mike and me actually then uh proximal policy optimization the most widely used algorithm today in that space and used as part of chat gpd's training so it's a real pleasure to have John back here with us I'll tell you one quick story of uh my own first encounter with John my own first encounter was not directly Charmed was Professor Jose carmena comes to me and he says he works in neurosciences there's this new student that I really want to recruitment absolutely the best this is the person I want to recruit um he wants to work on Prosthetics and Robotics is going to play a part in that can you please help me recruit him um I helped because hey Carmina recruit John next thing we know John is working in my lab I feel very very very very guilty I go to Jose I say Jose what do you think if John stays in my lab who says please he seems way more productive in your lab yeah yeah you have my blessing uh go for it and uh yeah thank you John so glad to have had you and uh thanks for making it back here floor is yours yeah thanks so much for the very kind introduction Peter uh yeah it's really great to be here uh back in my alma mater uh yeah I worked with uh work with theater on started out on working on Robotics and then uh got interested in reinforcement learning Midway through my PhD as a deep learning was starting to take off and uh that turned out very well and uh since um most of my time at open AI I've been running the RL team which uh switched a few years ago to the reinforcement learning team which uh switched to focusing on language models and fine-tuning them a few years ago and uh that led to some of the projects I'm going to talk about today  like a short answer question that you can uh like where you can uh if you could turn it into a a problem of predicting a single token like that the model is going to put a reasonable probability distribution on that token so that means it knows its uncertainty and it would be extremely surprising if it turned out that like the model can output a reasonable Distribution on that token but it has like no introspective access to like the uncertainty uh so that would be extremely surprising if it if it could do the task but it had no like but it couldn't introspect on its uncertainty and in fact there were a couple papers that sort of that studied that that I cited at the bottom that found that like the model you can get models to express their uncertainty in words and give similar results to the probabilities that they're outputting uh so okay so uh yeah so my claim was that models do know about their uncertainty and um and I think you can um we can fix uh and I also uh I claim that behavior cloning does the wrong thing but I would claim that RL actually does the right thing so um so first of all I mentioned a few thing problems that are a few like types of hallucination are just because the model uh is uh stuck in this pattern completion mode or it doesn't know how to it doesn't know it's allowed to express uncertainty uh so I think um oops I think that's pretty easy to fix like you can uh if uh if you just train the model uh with some examples where it's stating I don't know or it's expressing it's saying uh I don't have knowledge after that date uh or it's challenging the user's premise actually I don't think you're um that's true at all like if you train on a little bit of that data then the model is at least allowed to express uncertainty it just might not do it in exactly the right place and uh and I think RL basically is capable of learning the correct uh boundary of of when you should uh uh or basically RL is capable of learning like uh when you should say that I don't know and how much you should hedge so uh basically what we want uh conceptually this is not something that you can actually Implement is like this like you have um if you have a let's say an answer X um uh you have um you have like um uh you get a high reward if it's like a fully confident unhed correct answer uh like a little bit worse reward for a hedging 





 gbd three level models and I think but I'd say it's still this kind of thing is still useful for gbd4 for going even to more technical esoteric topics so the way the system works uh which I think is still relevant for gbd4 and we're still using uh is um is we actually Define this whole uh action space or DSL that the model can use to uh uh like to manipu like to browse its uh sources so um it so the the model has uh actions uh search it can do a search when it does a search it sees a list of Link links with little Snippets like a search page I can click on links uh they can quote things so um so the like basically with language models have a limited context window something like 4 000 tokens uh each token is about one word so you can't just if you're going to look at a lot of material like you can't you're going to run out of space so uh so quoting is really important uh we we're going to have to throw away uh like we're only going to be able to show uh like um uh show these pages to them model like briefly and then we're going to have to move it out of context so we allow the model to quote the content and that saves it for the rest of the browsing process um and then like once so so you have some browser operations and then the model is uh when it's done it can say I'm done and then it can write its answer um so that's um yeah that's the way that we so we just defined an RL environment like that where the model emits uh Tech it emits text it's not emitting like special actions uh but the text defines a DSL um uh so uh yeah we would basically uh have this um browsing the way the each episode of the RL task looks is the model browses for 20 to 100 steps It quotes a few things then it writes an answer and then the reward is computed with the reward model and we used some basic some standard methodology for this um and uh oops uh and yeah the training uh I haven't talked much about the the pipeline for RL from Human feedback but here's a picture of it uh you you first do Behavior cloning that's the supervised learning part uh you have expert demonstrations on how to do the task in this case using the browser and writing answers so we imitate that and then we collect reward model like we collect comparisons where we have the model output uh to uh in this case two trajectories or two whole answers A and B and we have a human decide which one is better and then we can either do RL on that reward model or we can do search against it um like take multiple samples and re-rank them so uh yeah we have we have to make these guis for each of these things so for collecting the data we we had some GUI that looks like that and for reward modeling we have like uh we have to get people to read the um like read the model written responses very carefully so here uh they see like they see this answer and they're gonna like highlight statements that have strong and weak support we had a pretty complex uh UI for this uh we don't I'm not like sure exactly how necessary all this stuff was but uh we decided to go overboard on like uh defining a really uh detailed process that people should go through to compute the factual accuracy of the answer though at the end of the day uh after they go through this process of highlighting everything we just get a binary we get one bit of information at the end and uh we tried using all the other information and it didn't help very much so uh that that's one disappointing thing um okay so how does does it work um so yeah we found um uh so this plot on these plots on the left are actually uh best of n meaning uh for a given query you take n samples you re-rank them with the reward model and you return the best one uh so there's no like you're not fine-tuning the uh and we take we use the um the policy from supervised learning not we don't train it with RL so uh yeah so we found that um we could like for the biggest model uh the this is gpd3 the classic uh gbd3 uh and the most samples 64 samples uh we could do better we got like we could beat the human demonstrators it was like preferred 55 to 40 percent of the time like um on like a little worse on coherence but better on factual accuracy um and we were also preferred a bit over the reference answers which were written by redditors um but uh actually I don't totally believe that that comparison I think there's uh like um I think sometimes uh like people prefer the model writes things that sound very definitive and have all these nice square bracket citations and uh yeah those are uh like even though we didn't tell our labeler I think we might have even stripped some of the citations out but the label is just really like how the style of the answers and I think that bias is the comparison unfairly so I I didn't believe that this was actually better than the top off-voted Reddit answers so I think uh probably if we ran this again with our current models it would be better um so now we actually have um we have so we have a alpha product in chat gbt which does browsing which is kind of using the exact same uh same like uh actions same sort of methods so I asked who's presenting at the Berkeley cloaking uh today I asked this this morning uh it says today's presenter is John Schulman blah blah uh it has um yeah so so that was that was that and if you look at the debug window you can see uh the model is being uh prompted with some long series of instructions about uh you have the tool the browser tool with these functions search uh quote back and it describes like the documentation for each of the functions uh and uh and then if uh like if you look at the conversation that's being generated uh the we see user message who's presenting at the colloquium assistant uh that's the AI it actually outputs an inner monologue as it's doing each of these actions so it says uh I will search for presenter at the Berkeley exclusive today it's not very useful but uh yeah it tells you what it's thinking it issues a Search Command Berkeley X colloquium presenter today uh recency days equals one we use Python syntax now so uh yeah it's um so yeah it does that it clicks it says let's click on the first link to access the department colloquium series page for excite UC Berkeley so it's giving you its inner monologue then it does the click action and uh yeah so then finally after it it quotes It quotes the relevant passages and then it finally writes its answer um so that's what browsing looks like now um I'd say one uh okay there are other um there are other things out there that do browsing uh like there's other products that do browsing now and have similar citations actually I'd say the one thing I'm uh one thing I uh I think is um special about this um is that it actually doesn't always do browsing it only browses when it doesn't know the answer so uh and and uh I think that uses the same kind of uh self-knowledge of uncertainty that I was describing uh earlier the same thing that allows the model to say I don't know allows it to realize it should only browse when it needs to so I asked what is the dagger algorithm so dagger is this uh kind of classic uh algorithm for imitation learning um so okay yeah it gives a uh like a detailed answer it doesn't browse at all then I looked at the bare blog and the top uh the first post was about something called Fleet dagger so I asked what is fleet dagger and now the model doesn't know what Fleet dagger is so it uh it goes and does a search then it looks at the sub web page which is actually the full archive paper and then it writes uh it writes some summary of the fleet of what Fleet dagger is which yeah which which I I verified is actually a summary it's not like a it didn't just copy and paste the whole thing but it's it yeah it just rephrase it a little bit okay so that's um that's all for that part of the talk um I I'm at it's at six o'clock now so I'm going to wrap up pretty soon I wanted to talk a little bit about uh open problems that I see in this whole line of work um so I'd say um so I'd say one big open problem is uh is just how to incentivize the model to really accurately express its uncertainty in words and that means uh the using the right amount of hedging uh and uh yeah proper like explaining um just explaining its full state of knowledge as well as possible um and I don't think our so our current um reward model methodology I don't think it does exactly the right thing like I was describing before it doesn't uh actually measure how much better one answer was than the other it's sort of just how confident is it that one is better than the other so yeah it we train the reward models with maximum likelihood uh like where probability that a wins over B our our model is the probability that a wins is proportional to like exponential of the uh reward score difference so yeah it's just we're doing a kind of um this is just like a classify like classification loss and um uh yeah so it doesn't uh like penalize uh like um it doesn't penalize the model for making extra confident errors it doesn't like account for hedging and everything so I think there's probably some effect where uh like if if um like an unhed wrong answer will be judged as worse than a hedge one but uh I I think yeah I don't think we're scoring things exactly right um and uh I'd say um it's actually like uh it's not clear exactly how to um okay but if you wanted to actually train with a let's say you wanted to train with something like a proper scoring function like you want that to be your reward uh like let's say we ask the model to Output probabilities on everything like uh it says 10 on this sentence 20 on this sentence uh that would also have some problems because uh natural language is just very imprecise and that's what makes it powerful but uh it's um there's like uh just as much fuzziness on the sentence as whatever probability you're like depending on how you interpret the sentence like how you would make it there's some like underlying interpretation of it and but there's so many possible interpretations some would have low probabilities some would have high probability it's uh that makes it very hard to do this um so yeah uh so I think this is a open problem maybe we should have some kind of formal statements of probability alongside the natural language statements but uh I don't know exactly how to do that or maybe we should uh like have some kind of uh set up some kind of objective where you have multiple agents collaborating and like they should Express 




