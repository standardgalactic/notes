
process physics doesn't give a reproduction formula for empirical data like our conventional physics does but it basically simulates the process you ala T of nature by modeling it's be able x' that is by modeling nature's events and their relational crosstalk so to say and all of that by means of a relational moana dalla ji next in line is that mainstream physics typically tries to reduce all of nature to inert bits of matter in motion whereas process physics treats nature as an integrated web of organismic relations finally because of its belief in reductionism mainstream physics treats conscious experience as an epiphenomenon or even as entirely illusory in process physics however subjectivity is at romÃ¡rio inherent aspect of nature that is the emergent activity patterns in the process monad ology seem to come where they collectively grow preference of how to connect among each other so this can be thought of as proto subjective and biomimetic because it seems to exhibit the same selectional connectivity as for instance in neural networks and slime all foraging patterns so mainstream physics separates the subject side from the target side of observation something which are in Schrodinger expressed like this without being aware of it and without being rigorously systematic about it we exclude the subject of cognizance from the domain of nature that we endeavor to understand we step with their own person back into the part of an onlooker who does not belong to the world the world which by this very procedure becomes an objective world now this method has worked very well for small subsystems of nature like Galileo's favorite object of study brass balls but it breaks down when trying to extrapolate it to nature as a whole because it cannot place ourselves in our measurement equipment outside of the universe instead of being outside our target of observation we are in fact inside the world to be observed or actually seamlessly embedded parts of it active participants in a world just as active as we ourselves coworkers in the process of nature in stark contrast our familiar method of external observation basically treats the world as a collection of external objects this however does not reflect how nature hangs together as one integrated process and therefore it comes with a fair amount of problems the first problem already appears quite soon in the measurement stage mainstream physics starts out by singling out some process of interest P separate from the measuring instrument and the observer but ultimately it's impossible to determine where the actual separation between the target and subject side is to be drawn it can be drawn between process and measuring instrument between instrument and observer between the eyes and the brain between the brain and a perception of process P or even between a perception of process P and its physical equation which is arguably located in the abstract world of mathematical ideas now despite the serious ambiguity of the target subject split it still stood the test of time and from Galileo onward has given us many empirically adequate physical equations it worked so well in fact that is often forgotten that our measurement data and sensory data are actually not fully complete carbon copies of the target of observation itself but because of the success of expressing our observations in terms of numerical data we started to think of observation in an info computational manner as the mirror registration of mathematically tractable computational information accordingly when we look at this linear measurement sequence from the left hand side it can be likened with classical information theory going from information source to recipient and when we look at the right hand side of the sequence from measurement instrument to physical equation we can recognize the format of algorithmic information theory which is all about compressing empirical data into suitable algorithms a hidden side effect of this linear info computational methodology is that our numerical data mathematical equations concepts categories symbols and names are typically Sonata mised with their target process P or with whatever it is that we decided to call P but this Sonata me between the two sides of the target subject split can only be imagined to exist after the split has been drawn on top of that the empirical agreement between measurement data and physical equation must always be forced by us on the basis of subjective decision-making within the measurement practice that is we subjectively choose which data points deviate too much and should be left out which observable quantities should be included in our physical equations which statistical formats should be employed etc etc so despite what this linear setup might imply our physical equations are not exact objective representations of some target process p werner heisenberg already mentioned this long ago when he said that what we observe is not nature in itself but nature exposed to our method of questioning always with an element of subjectivity this effectively means that our targets of observation do not objectively exist separately and independently from our means of observation this doesn't of course make all electrons and other elementary particles illusory or pure fabrications of the mind but it does mean that our concept of an electron is better thought of as a context-dependent figure of speech or a fad for all practical purposes as John Stuart Bell used to put it so at the end of the day it is a figure of speech for how the process you ality of nature plays out through the lens of empirical experience so the linear sequence of info computational representation is actually quite misleading after all no external representation will ever be able to give an exact one and once anatomy with the target side therefore it would be better to look for a method that does not portray the target side from the subject side of the split semiotics which by the way includes biosemiotics seems to be much better equipped to give a view from within that is it enables us to look at the empirical experience not as a linear sequence but as a cyclic process well embedded within the greater process of nature itself hence we do not get to know nature by taking in empirical and sensory data from the outside of it but instead we make sense of nature from the inside by living through it therefore the process of going through a semiotic cycle is a far better way to depict scientific observation in a preparation part the target process is singled out or soaked loose from its embedding environment after which it can be submitted to observation further on down the line where it will have its impact on the sign interpreting observer furthermore the formalization part of the semiotic cycle deals with the ever more precise formulation of physical equations going into more detail the formalization process runs approximately like this there's a table of raw measurement outcomes it is typically identified with system states 1 2 3 and so on thus basically making them synonymous with the natural system that we are trying to capture mathematically it is supposed that the system evolves from one state to the next we can then try to model this with an algorithm accordingly possibly relevant member parameters have to be chosen the raw data have to be encoded into a carefully selected algorithm that fits these data after which this algorithm must be decoded again to check for empirical agreement with any future data when we put the subs subsequent system States on top of each other we get a sequence of system States or data entries accompanied by the outcomes produced by algorithm this sequence can then be interpreted as a time series and whenever a mathematical relation is found that agrees well enough with the past present and future empirical data we can safely speak of a well matured physical equation like for instance Newton's second law or Galileo's equation here oh and all a lot of subjective choices involved regarding which parameters to include into the algorithm choosing acceptable margins of error choosing which criteria should be the leading ones when it comes to algorithm choice algorithm selection and which statistical format to use etcetera etc however once a physical equation has reached full maturity and social acceptance the entire semiotics cycle and especially the role of subjective choice is banished to the background so what is left is basically the idea that nature can be equated or synonymous with the data and algorithms that come from empirical observation and both the semiotics cycle and any post algorithmic interpretation are typically pushed to the background in favor of no interpretation at all or in other words the so called shut up and calculate approach also known as instrumental ism like this mainstream physics simply becomes the act of turning all of nature into mere mathematics and it basically reasons away all acts of interpretation that ultimately make this mathematize ation possible at all likewise it also renders irrelevant the feedback loop that runs from post algorithmic interpretation to inspire the context of views with new possibilities all in all this denial of pre and post algorithmic interpretation allows mainstream physics to declare itself as fundamental by portraying nature as a real-world equivalent of its mathematics but in reality it is always preceded by pre algorithmic interpretation and subjective choice from this alternative perspective this pre algorithmic interpretation or metaphysics which is parts partly based on intuition and creative decision-making should then automatically be considered more fundamental than the physical equations that it helps put together in other words although mainstream physics typically supposes that our physical equations should ultimately be able to spell out nature in its full entirety it overlooks the fact that it's mathematical formulations can never be fundamental because there is always pre algorithmic interpretation that necessarily precedes it this Den tells us that the mathematical map should not be confused with the territory to be mapped just as observables cannot ever reach the status of be able so what does process physics offer as an alternative to avoid all these problems in order to illustrate this we can look at a possible scenario for biological evolution - coming into actuality of life from a prebiotic soup such a primordial soup can initially be a rather low grade background with nothing much happening but according to Stuart Kauffman and others an autocatalytic network again spontaneously sprang into being under the right circumstances for instance when there's enough chemical diversity within the superfluid Kaufmann holds that a collectively autocatalytic network can spring up from the lower great background after which further complexification can occur now in the process physics model something similar happens from an initially under free undifferentiated ground level of irregular noisy activity patterns a network of higher-order activity can lift itself into actuality through collective low-grade background process you allottee now mainstream physics claims more or less that our natural universe is called into being by laws of nature that are just given with no deeper origins of their own process physics on the other hand suggests that Nature has come into actuality from a largely undifferentiated background from a void like pre space that initially has no explicit connectivity or pattern formation to it this can be interpreted as a primordial vacuum like state like in this picture closer up however this vacuum like state may be seen as a fiercely fluctuating ocean of potential which contains all of existence in latent form this is actually very similar to what is believed in quantum field theory so we now know that proces physics does not start out with pre-existing laws of nature but with routine of nature it does so by setting up a modded ology matrix in which an initially panelist network of relations gradually starts to exhibit more higher-order pattern formation here we can see that this routine is written as an iterative update routine that indexes connection strength in the relational matrix that makes up the Monodology so this relational matrix forms an indexical grid of emergent connectivity with each iteration the update routine not only gives rise to the kind of system-wide cross connectivity but it can also be thought of as letting its connectivity strength all over the system fluctuate ever so slightly in the noisy manner this reflects the partial uncertainty about what is going on elsewhere within the system so like this the relational matrix starts evolving from an initially patternless pre geometric vacuum-like stage and as the matrix runs through its update cycles again and again each time it basically adds a layer of noise over all individual connection strengths within the matrix the preceding connection strengths of all member nodes in the relation matrix are represented by bij old which is the president's term looking like this from up close following two terms are the binding term or cross linkage term which can be thought of as a realization of machs principle and the novelty infusing noise term these two terms mostly cancel each other out but in the long run there will be enough reactive low-grade activity patterns to enable the emergence of a complex layout were branching network of higher order process structures as is illustrated on the extreme left hand side well at least the humble beginnings of it as the system goes through its iterations the precedents turn the cross linkage term and the noise term together will give rise to a constantly fluctuating landscape of connection strengths some entries become large because of the effect of the iterative update routine these large connection strengths tend to hook up together to form islands of elevated connectivity like this and when considering only the islands with large valued connections we can submit them to statistical analysis and see what global pattern there is to tower behavior the statistical analysis basically amounts to counting the total number of member notes in those islands then pick one reference node and see how many neighbors are nearest neighbors how many second nearest neighbors how many are three connections away and so on now the distance to strengths ratio for connectivity notes tells us that the overwhelming majority is formed by weak short distance connections and only a minut fraction is made up by strong long-distance connections in the islands of elevated connectivity this translates into short distance local connections being the most probable ones which then automatically leads to tree graph like branching structures as displayed above the table since this will be the most probable configuration that will occur the connectivity nodes within these branching structures will most likely organize themselves into a near three-dimensional distribution relative to one another that is the branching structures will end up getting the same distance distribution among their nodes as uniformly arranged points in a 3-dimensional space this can be found because the number of neighbors for our chosen reference node turns out to increase in proportion to the square of the number of steps away and this phenomenon is elsewhere only seen in three-dimensional spaces so what we get here is emergent three dimensionality from initial known connectivity driven by iterative stochastic routine the different branching structures basically grow to become embeddable in 3d as cell like sub networks looking somewhat like this although this artistic impression should be a bit more realistically looking what is also quite remarkable and should not be forgotten is that growth and decay of network connections occurs as the continuous addition of noisiness strengthens and weakens earlier growing islands of connectivity and their branching structures as this goes on for long enough each locality within the network will become so much correlated with the rest of the system that it acquires a local sense of how to contribute to global system preserving criticality is then saves the system from falling into chaos or at the other extreme end getting stuck in static water like this we may say that there is a local sense of global systemic self-preservation which can be seen as a very rudimentary sense of subjective decision-making through selectional connectivity as such the network as a whole can be seen to have directionality that is it exhibits a tendency to develop towards increasing complexity process ecologist Robert Ilana likes to call this ascendancy a feature which is also characteristic of autocatalytic networks and ecosystems you although this is a short time lapse of the surface of the Sun it illustrates quite well how to sell like branching structures may act together as an integrated dynamic network with individual cells coming in and out of actuality mind you this is just an analogy to help visualize things next to this complexity seeking behavior the network exhibits emergent relativistic inertial and gravitational effects emergent near classical behavior creative novelty inherent time like process well 'ti with open-ended evolution and many other features that we can also find in nature itself so to recap the process physics model evolves firstly from an initially patent l'esprit space where there is no manifest patterning towards a three-dimensional early universe with a relatively uniform distribution of matter in the long enough run then this will eventually take on the shape of a complex neural network like cosmic web when starting with the Cosmic Microwave Background beyond which we cannot make any observations it evolves like this running from initial near uniformity to the current situation of a neural network shaped cosmic web in animated motion the neural network like organization of the universe at a super galactic scale looks as follows this is a short piece taken from the Millennium simulation which is a large scale supercomputer simulation of the universe you from all this we can draw several conclusions namely process physics is an utterly ecological way of doing physics and that it does not split up its universe of discourse into target system subject system and their external environment furthermore process physics is organismic rather than mechanistic mutually informative rather than based on mere numerical data it aims to set up a more be able like modeling of nature instead of portraying its observables also via what may be called routine of nature it is habit establishing instead of being governed by pre available laws of nature and on top of all that it is also noise driven so that creative novelty becomes possible as an alternative for strict determinism all this however does not mean that we should get rid of our conventional nature dissecting physics as soon as possible instead I'd like to make the case that we need a binocular physics in which our conventional way of doing physics in a box and process physics become each other's frame of reference mainstream physics when it tries to mathematize are designated natural systems should be checked against what process physics has to say just as process physics has a modeling of nature as a whole should check its results against our conventional physics finally then process physics is based on what may be called foundations without foundation that is the process physics monadology models nature as if it were an emergent autocatalytic network that is lifted into actuality by the collective mutually upward nudging fluctuations of little level stochastic background activity so instead of being a limited to modeling nature at the level of observables as our contemporary physics does prosess physics is basically a large step closer to a be able like modeling of nature and this may very well free us from a lot of persistent troubles in the foundations of physics thank you very much for your attention if you have any questions just drop me an email using the address below have a nice day bye bye