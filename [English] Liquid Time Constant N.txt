introduce ramen hasani  so ramen finished his  bachelor in uh japan and he got to  poly me technically milano uh  as on on an award  of exceptional student i was fortunate  to  be able to recruit him afterwards uh  because he was working on  neural networks actually he was  implementing the hardware  in milano and so i  was able to to recruit him in my group  where he did an outstanding job and now  he is a postdoc at mit in the group of  daniel arras and i'm sure he's  continuing  his great activity over there so rameen  the floor is yours  thank you so much radu um all right hi  everyone  i'm going to um i'm going to describe  today  um a kind of technology we have been  working on  with radu and daniela for a couple of  years now  and this is a class of continuous time  neural networks called  liquid time constant networks so  let's get started by knowing by defining  what is a time continuous neural network  so um in time continuous neural network  determined by a flow of hidden states  x and its flow  is identified by a neural network f  number of  which is given by a number of layers n  and then  the width k and a type of activation  function  and then it's a function of its hidden  states that means it can have  feedbacks it has it's a function of its  input  and it has model parameters that so with  this kind of  terminology it can give rise to  dynamical systems  functions that we previously wouldn't be  able to  produce with normal normal neural  networks  so it was recently shown that um  they can be equivalent they can be the  continuous  depth equivalent of a residual network  that  um basically in a residual network or a  typical neural network we have um a  finite transformation  of computation graph which is  equidistant  basically as at each layer we are  computing  basically an output for for a neural  network  but then when you use a time continuous  neural network  you can have adaptive computation and  you can transform the space into a  vector field  where you can actually have a very  efficient computation  at arbitrary point based on the type of  numerical ode solver that you use  so to elaborate a little bit more about  the features of time continuous networks  let's define  let's see how a standard recurrent  neural network  discretized like a discretized version  of a continuous flow  which is um which was developed in the  90s and then  so this is actually the transformation  that you can have  computing your next value given a neural  network  that has recurrent connections inputs  it's a function of time  or inputs input sequences and it's  parameterized by parameter teta  so this is actually the transformation  of an recurrent network  and then a neural ode or a time  continuous network  the representation becomes a  differential equation  and then a more stable version of this  is called a ctrn and a ctrn is basically  just the same differential equation with  a damping factor  tau that give rise to a more stable  neural network now if i just want to  show you like how  different they are in modeling time  series  i can show you like the top part which  is a discretized recarnal network  the power of prediction and  extrapolation of um  of a typical standard uh recurring  network  is actually looks like this and the  transformer  this transformation is much more smooth  when we are using  a continuous time recurrent network and  that's one of the benefits that we can  get out of these systems so um  how do we implement these time  continuous neural networks  given a neural network by this  differential equation  then we can you can take advantage of  numerical ode solvers  so we can write their equation we can  approximate them  their next value by for example an  explicit euler  kind of discretization and then we can  get the next value  based on based on this differential  equation given here  by defining this delta t now  this is called a forward pass the  forward pass computes for you the next  value  given the current state and the current  inputs to the system  so the forward pass can be identified by  any type of uh numerical integrator or  numerical ode solver  so this numerical choice of the ode  solvers actually  identifies the forward pass complexity  now how do we train them  so you can use a method that is called  adjoint  sensitivity method again um  every time we want to train a neural  network what we want to do we want to  define a loss function  that let's define our neural ode by this  function  and the flow of neural body e in time  is given by this figure over here  at every given point in time we are  being able to compute a loss  and then this loss can be computed by a  kind of a call to an ode solver  that receives your first input and  receive  that does have the neural network f and  then it wants to arrive at a final  point and then basically you want to sum  up all those losses that you compute  based on the loss function given loss  function to compute the loss value  so our joint sensitivity method wants to  take at every single point  run an axillary differential equation  this adjoint state  that is corresponding to the derivative  of the loss  in respect to the state of the neural  network  that means at every given point if we  run this differential equation  this would be a backward run to the same  ode solver  to basically compute the previous value  of the loss  so that you can you can compute their uh  the gradients  in respect to the loss with one step  and then at the like at the current step  you can do one more  gradient propagation and you would not  require to propagate back  throughout the whole sequence this will  give you  a complexity of one basically it's a  constant complexity  when you use a joint sensitivity method  then there is another method to compute  this is called back propagation through  time it's a classical method  where we have a forward pass and then  given a forward pass what you want to  compute you want to just  go through the ode solver like how we  are um  like uh discretizing this ode and then  um computing the the loss gradients  one by one and then using the chain rule  to compute your  uh gradients and then you can update  your old parameters by new parameters  so i showed you how to do forward pass  and how to do  uh backward pass which type of of these  trainings to use depending on the  situations  because the the black propagation  through time itself comes with a comp  a really really significant computation  but it has its own advantages  so the advantage of using um back  propagation through time  comes with an error that is caused by  the adjunct sensitivity method  imagine if i show you a continuous  vector field like that  and if i give you an initial value for a  system then you can run this system  then this is actually the the actual  forward run of the system  now for the adjoint time where we want  to  make the reverse transformation the  ideal transformation was to actually go  back  directly over this trajectory but  actually the adjoint  or the e-solver can can give rise  to numerical errors for the latent error  so and  in general just want you to show that  the memory it is true that we gain  memory complexity but um  um but we actually lose backward  accuracy  so i mean in cases where compute doesn't  matter  then you can actually stay with vanilla  back propagation through time  which is a better one now um  neural odes and defining  defining neural networks in terms of  differential equations  have a lot of advantages and in  principle in in theory  um they should be extremely good for um  kind of tasks that that deal with  sequential data  and continuous time spaces such as robot  control  uh um a variety of robot control tasks  that i'm showing here so  can a neuron but but so far we haven't  seen  in the literature that abnormal neural  ode  in the form of a recurrent network can  actually be  as expressive as advanced rnns in real  world applications  so there has to be a more expressive  way to show neural odes because with  this current form  they cannot beat the state-of-the-art  classical  recarnal networks so  with this motivation we decided to look  at the problem differently  so we gave rise to a category  of neural networks that their hidden  states  are actually computed by a very simple  linear ode  and this linear ode has a damping factor  or a  time constant tau is which is a constant  value  and receives a specialized input s  this specialized input s now is  identified by a neural network  multiplied by the difference of a  parameter  of a biasing parameter minus the state  of the neural network  so with this definition if i add  this s inside the equation one  what i can get i can get a more complex  representation of a  of an ode that has  a special characteristics for example  as we see here the coefficient of  x of t in this equation was only tau a  constant value  so and this coefficient actually defines  the characteristics of the differential  equation  given no input autonomous behavior of  the system  now the coefficient of x is defined by  the neural network itself  so that means at every given time point  in time we would have a different  dynamical system that is adaptable to  the environment so  defining the diff uh basically the  the state of the neural network would be  solution of this initial value problem  and then which has a variable time  constant  the neural network f also appears in the  state in defining the state  of the differential equation and also  inside  the time constant element or the  coefficient of x element  which is different and we recently  showed that this system actually comes  with  a lot of attractive behavior for example  if i  call this coefficient a time constant of  the system  and also defining the whole state of the  neural network x of t  we showed that even if the inputs to  this system goes to infinity that  the time constant of the system this  time constant of the system  for every nodes inside the neural  network  is actually stable and is bounded  between two  um like lower bound and after that  we also showed that uh the state itself  for every neuron or every node inside  these  liquid time constant networks or ltc's  they are they're also bounded so that's  also another nice  uh property of these um networks  another uh property of this network is  that like  i arbitrarily choose uh um  um chose um a kind of differential  equation  for myself and then is it a universal  approximate or not  can we approximate any kind of dynamics  by  uh by an arbitrary precision with these  networks  then we also follow um the kind of  principles of universal approximation  for neural networks  to actually show that this system also  has universal approximation capabilities  but then um where how  deep and how these neural networks  are basically more expressive like  universal approximation applies to all  types of neural networks  how can we distinct between like shallow  networks  and deep networks so for actually  digging a little bit deeper into the  foundation of  how expressive are these liquid time  constant networks  we use one of the nice definitions that  uh the deep learning community  actually came up with it's called the  extra it's called the trajectory lengths  um measure of expressivity imagine you  have a trajectory a 2d trajectory  circle and you want to input this  trajectory  inside a neural network a normal neural  network  and then what you have you have a neural  network with n layers  and with k and it's and its weights are  actually sampled from this end  then the observation was that if you  project  the trajectory input trajectory to this  system  layer by layer in this neural network we  see that the complexity of these  trajectories increases by the number of  layers  not only the complexity of this  increases  but also if we compute the lengths or  arc lengths of this trajectory  this arc length has a lower bound that  is exponential in the number of layers  that means  the the lengths here is your neural  network the more the  the the the deeper is your neural  network the more  expressive you might get based on these  trajectory lines  so that's a measure of expressivity that  sets the boundary between  shallow networks and deep networks so we  try to formulate the same thing  and apply this to a couple of  continuous time recurrent neural network  kind of  algorithms neural ode architectures  and ltc's and then we project back the  activity  into 2d for this uh  basically continuous time neural  networks and we observed that  continually  we saw that the trajectory lengths in 2d  for the ltc models are basically longer  and more complex  for example here we show that if you're  just changing the  weighting initialization of the of the  neural network  you can create extremely complex  patterns  by ltcnet networks while  the complexity and the lengths of the  trajectory  of the neural ode and  and ctrns are basically stays very very  uniformly  and then we also like try to  extend this to to actually see that by  changing the many variables  uh inside these architectures can be  actually keep  our uh interpretation of this system so  can can we can we actually prove that  these systems are more uh expressive in  terms of  um trajectory lengths and that's  actually the case so the yellow line  here in all curves  where we are showing into different ode  solvers different network  with then we have um initialization  weighting initialization  scale and also network layers  then um we also um  approved like some lower bounds for we  found some lower bounds for the  trajectory lengths  or the expressivity of neural odes  uh ctrns and ltc representations  and then based on that we actually  validated that our theory actually  works uh our and also our experimental  results are kind of validated so if  these systems are more expressive  in in theory then what would uh how  would they  uh compare against the state of the art  um  in in real world applications for  example if you look at the  like movement of cheetah we can see that  if you want to model that behavior then  um  ltcs outperform lscm policies  continuous time gre units and  neural odes as well if we  compare them in a real world application  of person recognition  uh like per person pattern like this is  this is a data set where  um it's like a time series of activity  of a human body and you want to classify  sequence of behavior  into certain pattern we see that ltcs  are also  kind of performant we also took  a couple of real world applications and  then we saw that in most cases  ltcs are outperforming other models  and now what i want to show in summary  so far  to you is that we introduced this model  we showed that it's universal  approximator  we showed that it has a stable dynamics  and also they show better degrees of  accuracy  expressivity and they can vary their  behavior  even post training because of the  liquidity  and um so um these systems are also good  for  irregularly sampled data one of the one  of the  um examples i showed you like  the tables we benchmark against this  irregularly  sampled data and they can actually model  continuous time processes pretty  effectively  but now can we scale the applications of  ltc  into even larger networks like because  the examples i showed you like  these are all small examples and  something with larger impact and maybe  even exploit  what is actually possible to do with  this type of modeling system  for example how can we apply this system  to um to um  end-to-end control of cars like  autonomous driving  where uh we collect a bunch of human  data and then we want to perform  at the driving activity so  in the current autonomous vehicles that  are camera based  for example tesla autopilot we have  some sort of like the inputs are rgb  images  they actually go through a kind of  cascade of layers of convolutional  networks  followed by fully connected layers that  actually  shape off shape up the whole thing as  like 20 to 100 million  parameters or let's say like you can  even scale it down from 5 million to  let's say 100 million parameters this  system can  actually drive a car end-to-end  then um what we actually try to ask  is that what happens if we can actually  replace the  fully connected layers where we have an  overload of parameters training  parameters  with recurrent neural networks and what  happens if we use ltc  based networks and then compare the  performance  of four different architectures where we  have lstms  as a control unit right after  convolutional layers  we have ctrns or neural odes basically  right after the convolutional layers the  convolutional neural network  and an architecture we call this just  based on a  network of ltc systems is a four-layer  specific architecture which we call  neural circuit policies which is built  by the ltc model  and we want to see what can be gained if  we actually  formulate this it turns out that to do  it  to do the autonomous driving in real  world  which we would need only 19 neurons of  ncp neurons  with as much smaller convolution on our  network  and much smaller number of trainable  parameters  that's one of the advantages of having  an expressive model  inside a larger scale problem  and other aspects of these networks is  um i would like to  talk over it like with this video so um  this is now a normal  um convolutional neural network with  fully connected layers  and it's fully connected layers are  showing here negativity of the fully  connected layers  while the driving is happening because  this is the input to the neural network  and neural network has to do  this decide about how to actually go  about the driving  we see that a convolutional neural  network this is actually what  is out there right now similar to this  architecture  is over the tesla autopilot so  what we have here on the on the  bottom here what i'm what i'm showing is  the attention  map of the convolutional layers  while the driving is happening in this  street  so we see that the convolutional neural  network is attending to this  roads sideways to to make a driving  decision  mostly that's the case when driving  happens if i add a little bit of noise  over this what we see  is that the attention map or the  decision of the networks  are not reliable anymore and that's one  of the fundamental problems it is true  that a convolutional network can  actually solve this problem  but also their decisions are not going  to be reliable anymore  instead of that if we actually use  um an ncp network which only has 19  neurons  basically what you get is an attention  on the roads horizon so basically it's  kind of capturing the main  causality of the data and also  even if i add noise on my data  look at the attention map it is  definitely  affected by the noise but what we see  is that its concentration is mostly  preserved so its decisions are more  robust compared to other networks  here what i'm showing you is a time  sequence of  inputs to a neural network with  lstm networks this is the attention map  of an lstm network  attention map of neural odes  convolutional networks and ncps  we see that there is actually a  consistently  focus on the horizon while we are using  an ncp network  and that's one of the very nice features  of these um  networks that we actually um explored  and found  um empirically  another thing that we found is that um  the number of  crash like imagine i on the x-axis i'm  increasing the amount of noise  and the y-axis i'm measuring the number  of crashes  that would happen in real world then we  see that ncb's  the ones that is on the lower side is  actually  much more resilient to perturbations  another recent work that we were  actually  looking at was in  what is the underlying principle like  can we actually mathematically talk  about  is there really like this liquid time  constant network ltc networks are they  really causal models  that's actually another work that is on  the revision right now where we actually  show  um um theoretically that  a system composed of let's say a  dynamical causal model  be two neurons let's say we have two  neurons and they they're connected  through these synopsis together  dynamical causal models  would uh allow inter intervention  for from outside world onto the synaptic  parameters  of two neurons that are interacting with  each other they have an internal  coupling  and they have basically a fixed  parameter that affect that  controls or tunes the inputs directly to  this  uh x i or the neural number two and then  if you have an  analogy of to the liquid time constant  networks  we see that we would have the same  type of constants but here we have a  kind of non-linearity  we have a non-linear control that is  controlled by neural network so a neural  network actually  controls how the interaction between two  neurons  are basically formed  and this is actually where the liquidity  modulator or a non-linear liquidity  modulator is actually coming up  and this this is what we found in the  paper then what we showed we showed it  on  [Music]  a task of drone navigation so what  you're seeing is that  this there is a drone that is learned  only by visual inputs to actually move  towards the target  so we see that on the right hand side  what we see is that  as soon as the drone realizes that there  is a target  all the attention of the ncp network is  actually comes on the  um on the target and that's actually  something that  came without even coding this and  without any inductive bias inside the  system  we also tested it on multi-agent system  so what i'm showing here  is a view from a first drone or a  follower drone  and the drone inside the environment is  actually a  leader drone and then  the the follower drone actually learned  how to attend to this um second run  while performing a task of leader  following  and now in the leader following task if  you look into the attention of the  network  like this is the view from the first  drone to the second drone which is here  we see that the attention of the network  is actually on  um on the first drone  and this is actually one of the uh  really cool observations that we could  connect  ltc networks as causal models  so they are actually causal models as  well  i would like to thank you for your  attention with this  and um would like to invite you i also  like to thank  my collaborators radhu daniela alexander  and matthias  and um so that we have we have open  sourced  all our uh kind of technologies you can  actually get access in both  uh tensorflow and python if you want to  get hands-on  and um with that uh thank you for your  attention  thank you i mean this was super exciting  uh i'd like to open the floor to  questions  the chat will work or a microphone will  work  so i have a question uh mr randall so  so why do you think these networks uh  are more expressive than let's say  classical neuralgi's  yeah so one of the things is that we use  this measure you know like we use the  expressivity measure  








































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































introduce ramen hasani  so ramen finished his  bachelor in uh japan and he got to  poly me technically milano uh  as on on an award  of exceptional student i was fortunate  to  be able to recruit him afterwards uh  because he was working on  neural networks actually he was  implementing the hardware  in milano and so i  was able to to recruit him in my group  where he did an outstanding job and now  he is a postdoc at mit in the group of  daniel arras and i'm sure he's  continuing  his great activity over there so rameen  the floor is yours  thank you so much radu um all right hi  everyone  i'm going to um i'm going to describe  today  um a kind of technology we have been  working on  with radu and daniela for a couple of  years now  and this is a class of continuous time  neural networks called  liquid time constant networks so  let's get started by knowing by defining  what is a time continuous neural network  so um in time continuous neural network  determined by a flow of hidden states  x and its flow  is identified by a neural network f  number of  which is given by a number of layers n  and then  the width k and a type of activation  function  and then it's a function of its hidden  states that means it can have  feedbacks it has it's a function of its  input  and it has model parameters that so with  this kind of  terminology it can give rise to  dynamical systems  functions that we previously wouldn't be  able to  produce with normal normal neural  networks  so it was recently shown that um  they can be equivalent they can be the  continuous  depth equivalent of a residual network  that  um basically in a residual network or a  typical neural network we have um a  finite transformation  of computation graph which is  equidistant  basically as at each layer we are  computing  basically an output for for a neural  network  but then when you use a time continuous  neural network  you can have adaptive computation and  you can transform the space into a  vector field  where you can actually have a very  efficient computation  at arbitrary point based on the type of  numerical ode solver that you use  so to elaborate a little bit more about  the features of time continuous networks  let's define  let's see how a standard recurrent  neural network  discretized like a discretized version  of a continuous flow  which is um which was developed in the  90s and then  so this is actually the transformation  that you can have  computing your next value given a neural  network  that has recurrent connections inputs  it's a function of time  or inputs input sequences and it's  parameterized by parameter teta  so this is actually the transformation  of an recurrent network  and then a neural ode or a time  continuous network  the representation becomes a  differential equation  and then a more stable version of this  is called a ctrn and a ctrn is basically  just the same differential equation with  a damping factor  tau that give rise to a more stable  neural network now if i just want to  show you like how  different they are in modeling time  series  i can show you like the top part which  is a discretized recarnal network  the power of prediction and  extrapolation of um  of a typical standard uh recurring  network  is actually looks like this and the  transformer  this transformation is much more smooth  when we are using  a continuous time recurrent network and  that's one of the benefits that we can  get out of these systems so um  how do we implement these time  continuous neural networks  given a neural network by this  differential equation  then we can you can take advantage of  numerical ode solvers  so we can write their equation we can  approximate them  their next value by for example an  explicit euler  kind of discretization and then we can  get the next value  based on based on this differential  equation given here  by defining this delta t now  this is called a forward pass the  forward pass computes for you the next  value  given the current state and the current  inputs to the system  so the forward pass can be identified by  any type of uh numerical integrator or  numerical ode solver  so this numerical choice of the ode  solvers actually  identifies the forward pass complexity  now how do we train them  so you can use a method that is called  adjoint  sensitivity method again um  every time we want to train a neural  network what we want to do we want to  define a loss function  that let's define our neural ode by this  function  and the flow of neural body e in time  is given by this figure over here  at every given point in time we are  being able to compute a loss  and then this loss can be computed by a  kind of a call to an ode solver  that receives your first input and  receive  that does have the neural network f and  then it wants to arrive at a final  point and then basically you want to sum  up all those losses that you compute  based on the loss function given loss  function to compute the loss value  so our joint sensitivity method wants to  take at every single point  run an axillary differential equation  this adjoint state  that is corresponding to the derivative  of the loss  in respect to the state of the neural  network  that means at every given point if we  run this differential equation  this would be a backward run to the same  ode solver  to basically compute the previous value  of the loss  so that you can you can compute their uh  the gradients  in respect to the loss with one step  and then at the like at the current step  you can do one more  gradient propagation and you would not  require to propagate back  throughout the whole sequence this will  give you  a complexity of one basically it's a  constant complexity  when you use a joint sensitivity method  then there is another method to compute  this is called back propagation through  time it's a classical method  where we have a forward pass and then  given a forward pass what you want to  compute you want to just  go through the ode solver like how we  are um  like uh discretizing this ode and then  um computing the the loss gradients  one by one and then using the chain rule  to compute your  uh gradients and then you can update  your old parameters by new parameters  so i showed you how to do forward pass  and how to do  uh backward pass which type of of these  trainings to use depending on the  situations  because the the black propagation  through time itself comes with a comp  a really really significant computation  but it has its own advantages  so the advantage of using um back  propagation through time  comes with an error that is caused by  the adjunct sensitivity method  imagine if i show you a continuous  vector field like that  and if i give you an initial value for a  system then you can run this system  then this is actually the the actual  forward run of the system  now for the adjoint time where we want  to  make the reverse transformation the  ideal transformation was to actually go  back  directly over this trajectory but  actually the adjoint  or the e-solver can can give rise  to numerical errors for the latent error  so and  in general just want you to show that  the memory it is true that we gain  memory complexity but um  um but we actually lose backward  accuracy  so i mean in cases where compute doesn't  matter  then you can actually stay with vanilla  back propagation through time  which is a better one now um  neural odes and defining  defining neural networks in terms of  differential equations  have a lot of advantages and in  principle in in theory  um they should be extremely good for um  kind of tasks that that deal with  sequential data  and continuous time spaces such as robot  control  uh um a variety of robot control tasks  that i'm showing here so  can a neuron but but so far we haven't  seen  in the literature that abnormal neural  ode  in the form of a recurrent network can  actually be  as expressive as advanced rnns in real  world applications  so there has to be a more expressive  way to show neural odes because with  this current form  they cannot beat the state-of-the-art  classical  recarnal networks so  with this motivation we decided to look  at the problem differently  so we gave rise to a category  of neural networks that their hidden  states  are actually computed by a very simple  linear ode  and this linear ode has a damping factor  or a  time constant tau is which is a constant  value  and receives a specialized input s  this specialized input s now is  identified by a neural network  multiplied by the difference of a  parameter  of a biasing parameter minus the state  of the neural network  so with this definition if i add  this s inside the equation one  what i can get i can get a more complex  representation of a  of a node that has  a special characteristics for example  as we see here the coefficient of  x of t in this equation was only tau a  constant value  so and this coefficient actually defines  the characteristics of the differential  equation  given no input autonomous behavior of  the system  now the coefficient of x is defined by  the neural network itself  so that means at every given time point  in time we would have a different  dynamical system that is adaptable to  the environment so  defining the diff uh basically the  the state of the neural network would be  solution of this initial value problem  and then which has a variable time  constant  the neural network f also appears in the  state in defining the state  of the differential equation and also  inside  the time constant element or the  coefficient of x element  which is different and we recently  showed that this system actually comes  with  a lot of attractive behavior for example  if i  call this coefficient a time constant of  the system  and also defining the whole state of the  neural network x of t  we showed that even if the inputs to  this system goes to infinity that  the time constant of the system this  time constant of the system  for every nodes inside the neural  network  is actually stable and is bounded  between two  um like lower bound and after that  we also showed that uh the state itself  for every neuron or every node inside  these  liquid time constant networks or ltc's  they are they're also bounded so that's  also another nice  uh property of these um networks  another uh property of this network is  that like  i arbitrarily choose uh um  um chose um a kind of differential  equation  for myself and then is it a universal  approximate or not  can we approximate any kind of dynamics  by  uh by an arbitrary precision with these  networks  then we also follow um the kind of  principles of universal approximation  for neural networks  to actually show that this system also  has universal approximation capabilities  but then um where how  deep and how these neural networks  are basically more expressive like  universal approximation applies to all  types of neural networks  how can we distinct between like shallow  networks  and deep networks so for actually  digging a little bit deeper into the  foundation of  how expressive are these liquid time  constant networks  we use one of the nice definitions that  uh the deep learning community  actually came up with it's called the  extra it's called the trajectory lengths  um measure of expressivity imagine you  have a trajectory a 2d trajectory  circle and you want to input this  trajectory  inside a neural network a normal neural  network  and then what you have you have a neural  network with n layers  and with k and it's and its weights are  actually sampled from this end  then the observation was that if you  project  the trajectory input trajectory to this  system  layer by layer in this neural network we  see that the complexity of these  trajectories increases by the number of  layers  not only the complexity of this  increases  but also if we compute the lengths or  arc lengths of this trajectory  this arc length has a lower bound that  is exponential in the number of layers  that means  the the lengths here is your neural  network the more the  the the the deeper is your neural  network the more  expressive you might get based on these  trajectory lines  so that's a measure of expressivity that  sets the boundary between  shallow networks and deep networks so we  try to formulate the same thing  and apply this to a couple of  continuous time recurrent neural network  kind of  algorithms neural ode architectures  and ltc's and then we project back the  activity  into 2d for this uh  basically continuous time neural  networks and we observed that  continually  we saw that the trajectory lengths in 2d  for the ltc models are basically longer  and more complex  for example here we show that if you're  just changing the  weighting initialization of the of the  neural network  you can create extremely complex  patterns  by ltcnet networks while  the complexity and the lengths of the  trajectory  of the neural ode and  and ctrns are basically stays very very  uniformly  and then we also like try to  extend this to to actually see that by  changing the many variables  uh inside these architectures can be  actually keep  our uh interpretation of this system so  can can we can we actually prove that  these systems are more uh expressive in  terms of  um trajectory lengths and that's  actually the case so the yellow line  here in all curves  where we are showing into different ode  solvers different network  with then we have um initialization  weighting initialization  scale and also network layers  then um we also um  approved like some lower bounds for we  found some lower bounds for the  trajectory lengths  or the expressivity of neural odes  uh ctrns and ltc representations  and then based on that we actually  validated that our theory actually  works uh our and also our experimental  results are kind of validated so if  these systems are more expressive  in in theory then what would uh how  would they  uh compare against the state of the art  um  in in real world applications for  example if you look at the  like movement of cheetah we can see that  if you want to model that behavior then  um  ltcs outperform lscm policies  continuous time gre units and  neural odes as well if we  compare them in a real world application  of person recognition  uh like per person pattern like this is  this is a data set where  um it's like a time series of activity  of a human body and you want to classify  sequence of behavior  into certain pattern we see that ltcs  are also  kind of performant we also took  a couple of real world applications and  then we saw that in most cases  ltcs are outperforming other models  and now what i want to show in summary  so far  to you is that we introduced this model  we showed that it's universal  approximator  we showed that it has a stable dynamics  and also they show better degrees of  accuracy  expressivity and they can vary their  behavior  even post training because of the  liquidity  and um so um these systems are also good  for  irregularly sampled data one of the one  of the  um examples i showed you like  the tables we benchmark against this  irregularly  sampled data and they can actually model  continuous time processes pretty  effectively  but now can we scale the applications of  ltc  into even larger networks like because  the examples i showed you like  these are all small examples and  something with larger impact and maybe  even exploit  what is actually possible to do with  this type of modeling system  for example how can we apply this system  to um to um  end-to-end control of cars like  autonomous driving  where uh we collect a bunch of human  data and then we want to perform  at the driving activity so  in the current autonomous vehicles that  are camera based  for example tesla autopilot we have  some sort of like the inputs are rgb  images  they actually go through a kind of  cascade of layers of convolutional  networks  followed by fully connected layers that  actually  shape off shape up the whole thing as  like 20 to 100 million  parameters or let's say like you can  even scale it down from 5 million to  let's say 100 million parameters this  system can  actually drive a car end-to-end  then um what we actually try to ask  is that what happens if we can actually  replace the  fully connected layers where we have an  overload of parameters training  parameters  with recurrent neural networks and what  happens if we use ltc  based networks and then compare the  performance  of four different architectures where we  have lstms  as a control unit right after  convolutional layers  we have ctrns or neural odes basically  right after the convolutional layers the  convolutional neural network  and an architecture we call this just  based on a  network of ltc systems is a four-layer  specific architecture which we call  neural circuit policies which is built  by the ltc model  and we want to see what can be gained if  we actually  formulate this it turns out that to do  it  to do the autonomous driving in real  world  which we would need only 19 neurons of  ncp neurons  with as much smaller convolution on our  network  and much smaller number of trainable  parameters  that's one of the advantages of having  an expressive model  inside a larger scale problem  and other aspects of these networks is  um i would like to  talk over it like with this video so um  this is now a normal  um convolutional neural network with  fully connected layers  and it's fully connected layers are  showing here negativity of the fully  connected layers  while the driving is happening because  this is the input to the neural network  and neural network has to do  this decide about how to actually go  about the driving  we see that a convolutional neural  network this is actually what  is out there right now similar to this  architecture  is over the tesla autopilot so  what we have here on the on the  bottom here what i'm what i'm showing is  the attention  map of the convolutional layers  while the driving is happening in this  street  so we see that the convolutional neural  network is attending to this  roads sideways to to make a driving  decision  mostly that's the case when driving  happens if i add a little bit of noise  over this what we see  is that the attention map or the  decision of the networks  are not reliable anymore and that's one  of the fundamental problems it is true  that a convolutional network can  actually solve this problem  but also their decisions are not going  to be reliable anymore  instead of that if we actually use  um an ncp network which only has 19  neurons  basically what you get is an attention  on the roads horizon so basically it's  kind of capturing the main  causality of the data and also  even if i add noise on my data  look at the attention map it is  definitely  affected by the noise but what we see  is that its concentration is mostly  preserved so its decisions are more  robust compared to other networks  here what i'm showing you is a time  sequence of  inputs to a neural network with  lstm networks this is the attention map  of an lstm network  attention map of neural odes  convolutional networks and ncps  we see that there is actually a  consistently  focus on the horizon while we are using  an ncp network  and that's one of the very nice features  of these um  networks that we actually um explored  and found  um empirically  another thing that we found is that um  the number of  crash like imagine i on the x-axis i'm  increasing the amount of noise  and the y-axis i'm measuring the number  of crashes  that would happen in real world then we  see that ncb's  the ones that is on the lower side is  actually  much more resilient to perturbations  another recent work that we were  actually  looking at was in  what is the underlying principle like  can we actually mathematically talk  about  is there really like this liquid time  constant network ltc networks are they  really causal models  that's actually another work that is on  the revision right now where we actually  show  um um theoretically that  a system composed of let's say a  dynamical causal model  be two neurons let's say we have two  neurons and they they're connected  through these synopsis together  dynamical causal models  would uh allow inter intervention  for from outside world onto the synaptic  parameters  of two neurons that are interacting with  each other they have an internal  coupling  and they have basically a fixed  parameter that affect that  controls or tunes the inputs directly to  this  uh x i or the neural number two and then  if you have an  analogy of to the liquid time constant  networks  we see that we would have the same  type of constants but here we have a  kind of non-linearity  we have a non-linear control that is  controlled by neural network so a neural  network actually  controls how the interaction between two  neurons  are basically formed  and this is actually where the liquidity  modulator or a non-linear liquidity  modulator is actually coming up  and this this is what we found in the  paper then what we showed we showed it  on  [Music]  a task of drone navigation so what  you're seeing is that  this there is a drone that is learned  only by visual inputs to actually move  towards the target  so we see that on the right hand side  what we see is that  as soon as the drone realizes that there  is a target  all the attention of the ncp network is  actually comes on the  um on the target and that's actually  something that  came without even coding this and  without any inductive bias inside the  system  we also tested it on multi-agent system  so what i'm showing here  is a view from a first drone or a  follower drone  and the drone inside the environment is  actually a  leader drone and then  the the follower drone actually learned  how to attend to this um second run  while performing a task of leader  following  and now in the leader following task if  you look into the attention of the  network  like this is the view from the first  drone to the second drone which is here  we see that the attention of the network  is actually on  um on the first drone  and this is actually one of the uh  really cool observations that we could  connect  ltc networks as causal models  so they are actually causal models as  well  i would like to thank you for your  attention with this  and um would like to invite you i also  like to thank  my collaborators radhu daniela alexander  and matthias  and um so that we have we have open  sourced  all our uh kind of technologies you can  actually get access in both  uh tensorflow and python if you want to  get hands-on  and um with that uh thank you for your  attention  thank you i mean this was super exciting  uh i'd like to open the floor to  questions  the chat will work or a microphone will  work  so i have a question uh mr randall so  so why do you think these networks uh  are more expressive than let's say  classical neuralgi's  yeah so one of the things is that we use  this measure you know like we use the  expressivity measure  
































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































