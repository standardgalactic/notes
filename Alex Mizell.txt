my responses to a few typical objections to what i am telling people about neural networks and machine learning.  these examples are taken from actual conversations, but the text has been changed to protect privacy of the people i was speaking with.

Conversant:  "I am a computer science major and you were not, therefore I understand about these networks and you are a fool."

A word on my experience.  I am 46 years old, and I have been working with computers since before I was 10.  I got my first computer job when I was 18, and I have almost never worked outside of computers since that time, for a total of about 25 years of professional experience in the field of computing.  Because I started my first IT job at 18 I never went to school for it, but that is not to say I don’t have a ton of training, on-site and in-classroom and certifications to prove that I learned things.  Every job I’ve had for the past 15 years has required a college degree, but they don’t hesitate to hire me.  I dare say I have much more experience with computers than your typical computer science newly-grad.  Among my fellows I am regarded as an expert in my field, which I would characterize as systems engineering.

On the subject of my AI knowledge specifically, I have been watching the development of AI for the past twenty five years with a great deal of anticipation.  I have read about perceptrons since I was a child.  I was playing with chat bots like ELIZA back when computers were all 8-bit.  I was coding for myself since then, too, and reading and understanding code since the 1980s.  I was watching closely when back propagation, transformers and convolutional neural networks started to make a big splash over the last decade as GPU’s got more powerful, and I was playing with Google Deep Dream at a time when practically nobody on the Internet had even heard of neural networks.  It is fair to say that I am a bit ahead of the curve on this one.  I have training in machine learning, I have coding experience in Python, and with PyTorch, and I have looked at the code of Stable Diffusion and other networks as well having created them from scratch in my own development environment.

Conversant:  "Latent diffusion models generate images from noise, therefore the output must be new and never-before-seen images."

The program starts with noise, but the noise doesn’t stay noise.  In many successive iterations the noise is pulled towards a particular visual output until you are left finally with only that output.  At that point there is no trace of the noise left – wouldn’t you agree?  So why does it matter at all that we started with noise?  If we start with noise and gradually lighten every pixel until we have a solid white field, would you say the white field “came from noise”?  Moreso it came from lightening the pixels.  It wouldn’t matter at all what you started with, the result certainly was not random.

Conversant:  "But what the model learned is only a statistical representation of the training set, not the training set itself."

Imagine for a moment that we are training a neural network on a single image – that of the Italian flag.  As you probably know already, this rectangular flag has only three colors – green, white and red, and they are laid out in three equally-wide vertical stripes left to right.  Notice that that previous sentence is an encoding, in simple English, of the image of an Italian flag.  Sometimes it really is that easy to encode images into a wholly different mode of communication.  If I asked you to draw an Italian flag based only on that description, you could do it easily, and it would look pretty much like every other Italian flag ever.  Image encoding and decoding can be as simple as that.

So what did we really do to create that English description?  Well, we had to notice the fact that there were three separate fields of color, that the colors were different, we had to note what colors they were, we had to know which one was on the left, in the middle, and on the right, and we had to know they were about equally wide.  These are the sorts of things that are encoded in the “statistical boundaries” – but you can see that they are all related to the way the thing looks, and nothing else.  These statistical representations are still all about colors and shapes and nothing else.

Now, when you store an image file as a compressed image, say a JPEG file, you are doing something very similar to this perhaps without realizing it.  While a bitmap file contains a single spelled-out color value for every pixel in the image, a JPEG file instead encodes “regions” of color.  Because now we don’t have to store the individual color values for every pixel, the compressed image file can be much smaller than the original file.  In the case of our Italian flag, the entire thing, regardless of how large the original size was in pixels, it contains only three regions of color.  If you encode only those three regions and their relative positions then you can decode an image of the Italian flag at any resolution you want, with nearly perfect accuracy, because all it ever was was a bunch of regions of colors.  And you know already that JPEG compression also works well even when the images are not as simple as this flag.  Encoding and decoding regions of color is just another way of saying that we encoded the “statistical boundaries” of the colors in the image.  Remember, an image is composed of absolutely nothing else BUT colors when you get down to it, and those colors tend to be correlated with each other in a way that makes some of them easily guessable like in this example.   Now realize that we can encode not only regions of color but also regions of texture.

So, the concept of storing images as a statistical representation is not something that only neural networks do – lossy image compression schemes do that too.  Sometimes it feels like laymen believe that the AI is doing something like automatically 3D-modelling the entire desired scene and then raytracing it to create an output image, but in reality the process is much simpler than that.  It’s all about re-assembling regions of color and visual texture that are encoded in an intermediate mode, like the English encoding of the Italian flag is neither the input nor the output, strictly speaking, but you can use it to re-assemble the output into a faithful re-creation of the input.

Conversant:  "But it generates the image from my prompt and noise!"

When you start talking about language models like CLIP and BERT, this only serves to distract from the fact that both the input (as training data) and the output are composed entirely of pixel color data.  The language model input (prompt) is used to create an intermediate description of the desired output, and then that is decoded into an image, but that obviously doesn’t work without the training having already taken place.  The training is absolutely critical to the process and cannot be dispensed with, because that is where the model gets its vocabulary of color regions and textures from which to assemble the output.  You can witness those pieces in an neural network using a feature visualization toolkit.  No, they are not stored as little bitmap images – they have to be decoded from the weights and biases – but that doesn’t mean they aren’t in there at all.  Even highly educated people seem to have a very difficult time imagining how colors and shapes are encoded into weights and biases, but the proof is in the pudding isn’t it?

Conversant:  "But the images are not stored in the model file as pixels."

If you like I can show you how you can store an image in a neural network and then reproduce it almost exactly from the model file as if it were functionally equivalent to an image storage and retrieval system.  Educated people told me that I couldn’t do that with multiple images, but I was able to do that with multiple images in the same model.   Given enough memory I think I could store an arbitrary number of images at an arbitrary quality level and retrieve them mostly intact.  This model will not generalize well, but it will contradict the statement that “the original works are in no way inside the neural network”.

Conversant:  "Overfitting can't happen in a 'good' neural network because then it would be a 'bad' neural network."

What I think some people fail to appreciate is that a model which generalizes well on some subjects may generalize poorly (be overfit) on other subjects, particularly those for which there were insufficient examples in the training data.  If our training set has a million different pictures of women and a million pictures of men and one picture of a dog, what sort of output do you think you get when you ask for a picture of a dog?  You get the one dog, regardless of how well it can generalize about men and women.

But I hear you say that steps have been taken to avoid this sort of poor training in the big commercial models.  There is a particular situation that happens very commonly in the real world which can lead directly to overfitting even in these models.  That is in the case of a named image – say a famous painting or a photograph.  You might have a thousand examples of the Mona Lisa, but nearly all of them will have this smiling woman in them, and nearly none of them will show the back of the painting.  And all of them will be tagged as being “Mona Lisa”.  So no matter how many examples you have of this painting, they will nearly all look the same, and tend to generate same-looking output.  The AI’s can’t generalize about this sort of thing because it is already a specific, well-known thing.

Examples of overfitting in Midjourney have been pointed out, and then they vanish some short time later.  It’s not because overfitting doesn’t exist in the training set anymore.  They did not retrain the entire network to make the example go away – that would be incredibly expensive.  They simply limit people’s ability to write prompts with those keywords, or they alter the output just enough to gain plausible deniability.  These hacks do not change the fact that there is a detailed “statistical representation” of the Mona Lisa “in” the model that in fact could be retrieved by someone that had direct access to the model file.  how else do you think it "knows" anything about the Mona Lisa besides encoding that entire image?

When you say “because they learned the brushstrokes, they learned the colors, the textures,” that is only to say that you do realize that the colors and textures are “in there” too.  What is an image if not purely colors and textures?  As far as a neural network is concerned, a dog is still just a set of colors and textures in the output, and that remains true even if that set of colors is strongly associated with another set of colors and textures ("features") that statistically represent a bone.  these representations may be associated with other, not-necessarily-visual features as well, but if it wasn't represented by some color and texture then it could never appear in the output, and it does.